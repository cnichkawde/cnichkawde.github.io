<!doctype html>

<head>
  <script src="template.v2.js"></script>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf8">
  <title>Beyond Transformers: Structured State Space Sequence Models</title>
  <style id="distill-article-specific-styles">
      .subgrid {
    grid-column: screen; 
    display: grid; 
    grid-template-columns: inherit;
    grid-template-rows: inherit;
    grid-column-gap: inherit;
    grid-row-gap: inherit;
  }

  d-front-matter {
    display: none;
  }
  
  d-figure.base-grid {
    grid-column: screen;
    background: hsl(0, 0%, 97%);
    padding: 20px 0;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  d-figure {
    margin-bottom: 1em;
    position: relative;
  }
  
  d-figure > figure {
    margin-top: 0;
    margin-bottom: 0;
  }
  
  .shaded-figure {
    background-color: hsl(0, 0%, 97%);
    border-top: 1px solid hsla(0, 0%, 0%, 0.1);
    border-bottom: 1px solid hsla(0, 0%, 0%, 0.1);
    padding: 30px 0;
  }
  
  .pointer {
    position: absolute;
    width: 26px;
    height: 26px;
    top: 26px;
    left: -48px;
  }
  
  div#observablehq {
    font-family: inherit;
    font-size: inherit;
  }
  
  button {
    font-family : inherit;
    font-size: 1em;
  }

  
  /* Alignment of KaTeX. */
  #gnn-models span.katex-display {
    margin: 0.5em 0 0.5em 0em;
  }
  
  @media (max-width: 1000px) {
    d-contents {
      justify-self: start;
      align-self: start;
      grid-column-start: 2;
      grid-column-end: 6;
      padding-bottom: 0.5em;
      margin-bottom: 1em;
      padding-left: 0.25em;
      border-bottom: 1px solid rgba(0, 0, 0, 0.1);
      border-bottom-width: 1px;
      border-bottom-style: solid;
      border-bottom-color: rgba(0, 0, 0, 0.1);
    }
  }
  
  @media (min-width: 1000px) {
    d-contents {
      align-self: start;
      grid-column-start: 1;
      grid-column-end: 4;
      justify-self: end;
      padding-right: 3em;
      padding-left: 2em;
      border-right: 1px solid rgba(0, 0, 0, 0.1);
      border-right-width: 1px;
      border-right-style: solid;
      border-right-color: rgba(0, 0, 0, 0.1);
    }
  }
  
  @media (min-width: 1180px) {
    d-contents {
      grid-column-start: 1;
      grid-column-end: 4;
      justify-self: end;
      padding-right: 3em;
      padding-left: 2em;
      border-right: 1px solid rgba(0, 0, 0, 0.1);
      border-right-width: 1px;
      border-right-style: solid;
      border-right-color: rgba(0, 0, 0, 0.1);
    }
  }
  
  d-contents nav h3 {
    margin-top: 0;
    margin-bottom: 1em;
  }
  
  d-contents nav a {
    color: rgba(0, 0, 0, 0.8);
    border-bottom: none;
    text-decoration: none;
  }
  
  d-contents li {
    list-style-type: none;
  }
  
  d-contents ul {
    padding-left: 1em;
  }
  
  d-contents nav ul li {
    margin-bottom: 0.25em;
  }
  
  d-contents nav a:hover {
    text-decoration: underline solid rgba(0, 0, 0, 0.6);
  }
  
  d-contents nav ul {
    margin-top: 0;
    margin-bottom: 6px;
  }
  
  d-contents nav > div {
    display: block;
    outline: none;
    margin-bottom: 0.5em;
  }
  
  d-contents nav > div > a {
    font-size: 13px;
    font-weight: 600;
  }
  
  d-contents nav > div > a:hover, d-contents nav > ul > li > a:hover {
    text-decoration: none;
  }
  
  input[type="radio"] {
    vertical-align: unset !important;
  }
  
  .math-details {
    padding-left: 1em;
    padding-right: 1em;
    padding-bottom: 0em;
    padding-top: 1em;
    margin-bottom: 1em;
  }
  </style>

  <style>
    /* Style the details element */
    details {
      background-color: #e6f7ff;; /* Light grey background */
      border: 2px solid #ddd; /* Optional: Add a border */
      padding: 20px;
      padding-top: 0;
      margin-top: 0;
      margin-bottom: 0;
    }
  
     /* Hide the summary element */
    summary {
      display: none; /* Hide the element */
    }
  
    /* Expand the content by default */
    details[open] > :not(summary) {
      display: block;
    }

    h1 {
      line-height: 1.2;
      color: #333;
    }

    .author {
        font-size: 1.5em;
        font-family: "Rubik", sans-serif;
    }

    .published-date {
        font-size: 0.9em;
        color: #777;
    }
  </style>
</head>

<body>
  <!-- <d-front-matter>
    <script id='distill-front-matter' type="text/json">{
    "title": "Beyond Transformers: Structured State Space Sequence Models",
    "description": "This article describes SSSSM",
    "published": "Jan 10, 2017",
    "authors": [
      {
        "author":"Chetan Nichkawde",
        "authorURL":"https://linkedin.com/in/cnichkawde"
      }
    ],
    "katex": {
      "delimiters": [
        {"left": "$$", "right": "$$", "display": false}
      ]
    }
  }</script>
  </d-front-matter>
   -->
  <d-article>
    <header>
      <center>
        <h1>
          Beyond Transformers: <br> Structured State Space Sequence Models
        </h1>
      </center>
      <center>
        <p><span class="author">Chetan Nichkawde</span></p>
      </center>
      <center>
        <p><time class="published-date" datetime="2024-01-15">January 15, 2024</time></p>
      </center>
    </header>
    
    <a class="marker" href="#abstract" id="abstract"></a>
    <p>
    In recent developments within the realm of sequence modeling, a groundbreaking paradigm has surfaced. This innovative approach demonstrates superior capability in accurately modeling extensive sequence lengths and contextual dependencies. Notably, it exhibits computational efficiency during inference, presenting a marked advancement over Transformer architectures. Additionally, this paradigm offers the advantage of a reduced model footprint, contributing to its practical applicability and scalability.
    </p>

    <a class="marker" href="#introduction" id="introduction"></a>
    <p></p>
  <details open>
  <summary><strong>Why do we want to go beyond Transformers?</strong></summary>
    <p>
    <h5>What makes Transformers great?</h5>
      The efficacy of self-attention due to its ability to route information densely within a context window, allowing it to model complex data.
    <h5>What are the challenges with Transformer models?</h5>
    <ul>
      <li>Inability to model anything outside of the context window.</li>
      <li>Quadratic complexity and scaling for training and inference with respect to the sequence length.</li>
    </ul>

  <h5>What makes State Space models great?</h5>
<ul>
<li>It has linear complexity with respect to the sequence length during the training time 
  which can easily be parallelized using parallel scans.</li>
<li>Many times faster during inference. It can do inference in constant time using the current state compared to Transformers 
  that requires computation of self-attention over the entire context.</li>
  <li>They can easily model very long range context and has 
    been shown to model a context length of about 1 million tokens.
    The Transformers cannot model long context due to quadratic scaling
  with respect to the sequence length.</li>
</ul>
</p>
</details>
 
<p style="margin-top:20px;">
The table below shows the results on Long Range Arena<d-cite bibtex-key="tay2020long"></d-cite> benchmark which has a set of tasks on data with long sequences.
</p>
<table style="width:100%; border-collapse: collapse;">
  <caption>
      <strong>Long Range Arena Benchmark</strong>
  </caption>
  <thead>
      <tr>
          <th>Model</th>
          <th>ListOps</th>
          <th>Text</th>
          <th>Retrieval</th>
          <th>sCIFAR</th>
          <th>Pathfinder</th>
          <th>Path-X</th>
          <th>Avg</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Transformer</td>
          <td>36.37</td>
          <td>64.27</td>
          <td>57.46</td>
          <td>42.44</td>
          <td>71.40</td>
          <td>&#10005;</td>
          <td>53.66</td>
      </tr>
      <tr>
          <td>Reformer</td>
          <td>37.27</td>
          <td>56.10</td>
          <td>53.40</td>
          <td>38.07</td>
          <td>68.50</td>
          <td>&#10005;</td>
          <td>50.56</td>
      </tr>
      <tr>
          <td>BigBird</td>
          <td>36.05</td>
          <td>64.02</td>
          <td>59.29</td>
          <td>40.83</td>
          <td>74.87</td>
          <td>&#10005;</td>
          <td>54.17</td>
      </tr>
      <tr>
          <td>Linear Trans.</td>
          <td>16.13</td>
          <td>65.90</td>
          <td>53.09</td>
          <td>42.34</td>
          <td>75.30</td>
          <td>&#10005;</td>
          <td>50.46</td>
      </tr>
      <tr>
          <td>Performer</td>
          <td>18.01</td>
          <td>65.40</td>
          <td>53.82</td>
          <td>42.77</td>
          <td>77.05</td>
          <td>&#10005;</td>
          <td>51.18</td>
      </tr>
      <tr>
          <td>FNet</td>
          <td>35.33</td>
          <td>65.11</td>
          <td>59.61</td>
          <td>38.67</td>
          <td>77.80</td>
          <td>&#10005;</td>
          <td>54.42</td>
      </tr>
      <tr>
          <td>Nystromformer</td>
          <td>37.15</td>
          <td>65.52</td>
          <td>79.56</td>
          <td>41.58</td>
          <td>70.94</td>
          <td>&#10005;</td>
          <td>57.46</td>
      </tr>
      <tr>
          <td>Luna-256</td>
          <td>37.25</td>
          <td>64.57</td>
          <td>79.29</td>
          <td>47.38</td>
          <td>77.72</td>
          <td>&#10005;</td>
          <td>59.37</td>
      </tr>
      <!-- <tr>
          <td><strong>S4</strong></td>
          <td><strong>59.60</strong></td>
          <td><strong>86.82</strong></td>
          <td><strong>90.90</strong></td>
          <td><strong>88.65</strong></td>
          <td><strong>94.20</strong></td>
          <td><strong>96.35</strong></td>
          <td><strong>86.09</strong></td>
      </tr> -->
      <tr>
        <td><strong>LRU</strong></td>
        <td><strong>60.2</strong></td>
        <td><strong>89.4</strong></td>
        <td><strong>89.9</strong></td>
        <td><strong>89.0</strong></td>
        <td><strong>95.1</strong></td>
        <td><strong>94.2</strong></td>
        <td><strong>86.3</strong></td>
    </tr>
  </tbody>
</table>
<p>
We can see that the results for Linear Recurrent Unit (LRU) discussed in this article far exceeds all the Transformers based models. Furthermore, the state space based models are 5 to 10 times faster on inference speed while requiring only 10% of the memory compared to Transformers. The longest context is about 16000 for the task Path-X. All of the Transformer based models are not able to solve this task while LRU has an accuracy of about 94%.
<em>
The purpose of this article is to develop a foundational level understanding on how structured state space sequence models work in efficienty modeling long sequences.
</em>
</p>

<h3>The quintessential spring-mass-damper system</h3>
<p>
We start our discussion with spring-mass-damper system which is one of the most widely studied system in classical mechanics and dynamics. The ideas from this template system has already been applied to various fields in science and engineering. For instance, if you want to understand the flight stability characteristics of a rocket flying in the atmosphere carrying large amount of liquid fuel, you can apply similar analysis performed for the rocket system linearized around an equilibrium point<d-cite bibtex-key="nichkawde2004stability"></d-cite>. <em>We will see later that the linear dynamical systems also form the backbone of the futuristic sequence models
which alleviates the challenges with Transformer models while offering same level of modeling fidelity</em>.
</p>

<d-figure>
  <img src="https://www.shimrestackor.com/Physics/Spring_Mass_Damper/Figs/2-cart.png" style="height:200px;width:500px;display: block; margin-left: auto; margin-right: auto;margin-top:10px;"/>
  <figcaption style="text-align:center; margin-top:10px;">
    <a class="marker" href="#springmass">
      Figure 1: A spring mass damper system with external forcing.
    </a>
  </figcaption>
</d-figure>

<p>
Shown above is an object with mass $$m$$ attached to a wall with spring with spring constant $$k$$ and a damper with damping coefficient $$c$$. The displacement of the object is represented by $$x(t)$$. The object is forced by an external time dependent force $$f(t)$$. The $$f(t)$$ we will later see equivalent to our sequential data. The spring $$k$$ is a potential energy store which is a converted into kinetic energy of the object in an oscillatory fashion. The damper $$c$$ usually dissipates the energy when $$c$$ is positive. A strong damping can quickly dissipate the energy leading the object coming to almost rest (remember vanishing gradients?!). However, $$c$$ can also be negative in which case the damper can actually absorb more energy from the environment and add it to the system eventually leading to explosion (remember exploding gradients?!). We will see later these dynamic stability characteristics have a deep connection with the modeling fidelity of our sequence model and we want to create inductive biases in the model formulation such that a similar linear dynamical system like above have good dynamic characteristics that facilitate learning.
<aside>The vanishing and exploding gradients can also be understood in terms of stability of dynamics of backpropagation. Here we are motivated by the need to find efficient parametrization to ensure just the right kind of dynamics during the forward pass.</aside>
</p>

<p>
Let us formulate the equations of motion for this system using the Newton's second law of motion:
<d-math block>F = ma</d-math>
where the force $$F$$ is
<d-math block> F = -c\dot{x}-kx</d-math>
The spring-mass-damper system also has a time-dependent driving force $$f(t)$$ which represents the input to the system and our sequential data.
Putting it all together:
<d-math block>m\ddot{x} = -c\dot{x}-kx+f(t)</d-math>
Now we express this equation in state-space form.
Let $$x_1 = x$$ and $$x_2 = \dot{x}$$.
Rewriting above:
<d-math block>
\begin{array}{l}
\dot{x}_1 = x_2 \\
\dot{x}_2 = -\frac{c}{m}x_2 - \frac{k}{m}x_1 + \frac{1}{m}f(t)
\end{array}
</d-math>
Expressing it in matrix form:
<a class="marker" href="#eq1" id="eq1"></a>
<d-math block>\dot{\mathbf{X}} = A\mathbf{X} + B\mathbf{U}(t) \qquad (1)
</d-math>
where
<d-math block>
\mathbf{X} =
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}, \quad \mathbf{U} =
\begin{bmatrix}
0 \\
f(t)
\end{bmatrix}
</d-math>
<d-math block>
A =
\begin{bmatrix}
0 & 1 \\
-\frac{k}{m} & -\frac{c}{m}
\end{bmatrix}, \quad
B =  \begin{bmatrix}
0 \\
\frac{1}{m}
\end{bmatrix}
</d-math>
</p>
<p>
  <em>
    The dynamical stability characteristics of any system expressed in above form, also known as state-space form, can be completely understood by inspecting the eigenvalues of the matrix $$A$$.
  </em>
    We want to have a postive damping to ensure stability. We can easily extend the above analysis to a multi-dimensional system.
    <aside>We want the system to be marginally damped but not overdamped from a deep learning perspective.</aside>
</p>
<p>    
Lets work out the formula for eigenvalues. The characteristic equation is given by:
<d-math block>
m\lambda^2 + c\lambda + k = 0
</d-math>
The roots (eigenvalues) are given by:
<d-math block>
\lambda_{1,2} = \frac{-c \pm \sqrt{c^2 - 4mk}}{2m}
</d-math>
When the discriminant $$\Delta = c^2 - 4mk$$ is negative, the eigenvalues become complex conjugates.
Let's consider the case where the discriminant is negative:
The natural frequency $$\omega_n$$ of the system defined as:
<d-math block>
\omega_n = \sqrt{\frac{k}{m}}
</d-math>
The eigenvalues can then be expressed in terms of $$\omega_n$$ as:
<d-math block>
\lambda_{1,2} = \frac{-c}{2m} \pm j\omega_d
</d-math>
where $$j$$ is the imaginary unit, and $$\omega_d$$ is the damped natural frequency given by:
<d-math block>
\omega_d = \sqrt{\omega_n^2 - \frac{c^2}{4m^2}}
</d-math>
</p>
<p>
  It is always possible to apply a coordinate transform such that the state transition matrix $$A$$ is diagonanlized with the eigenvalues as the diagonal entries<d-cite bibtex-key="axler1997linear"></d-cite>.
  Thus, we can get a decoupled set of equations of the following form:
  <d-math block>
    \frac{dx(t)}{dt} = \lambda x(t)
  </d-math>
  where $$\lambda$$ is the eigenvalue.
  To solve this ODE, we can separate variables and integrate:
  <d-math block>
    \int \frac{dx}{x} = \lambda \int dt
  </d-math>
  Integrating both sides gives:
  <d-math block>
    \ln|x| = \lambda t + C
  </d-math>
  where $$C$$ is the constant of integration.
  Taking the exponential of both sides:
  <d-math block>
    |x| = e^{\lambda t + C}
  </d-math>
  <d-math block>
    x(t) = e^{C} e^{\lambda t}
  </d-math>
  We can rewrite the above as:
  <a class="marker" href="#eq2" id="eq2"></a>
  <d-math block>
    x(t) = x_0e^{\lambda t} \qquad (2)
  </d-math>
  where $$x_0$$ is the initial state.  
</p>
<p>
Since the real part of $$\lambda$$ is $$\frac{-c}{2m}$$, it is easy to see from Equation (2) why we need a positive damping $$c$$ for the system to be stable. The exponential term will blow up as time progresses for a negative value of $$c$$ making the system unstable. We will revisit this equation in the Section <a href="#designingrnn">Designing linear recurrent unit.</a>
</p>
<p>
It is import to note that system represented by Equation <a href="#eq1">(1)</a> is linear. The matrices $$A$$ and $$B$$ do not depend on the state $$x$$ and the input $$f(t)$$. At this point, it would be good introduce another equation for the output $$y$$.
<a class="marker" href="#directy" id="directy"></a>
<d-math block>
  y(t) = C\mathbf{X} \qquad (3)
</d-math>
The output for the autoregressive model is the future value in time of $$f(t+1)$$. It would be the next token $$u_{k+1}$$ for a sequence model.
</p>

<p>
The data in the real world is always discrete especially if you are trying to models for discrete sequences like language, protein or DNA. The continuous time ordinary differential equation (1) can easily be discretized using methods like Zero-Order Hold or Bilinear transform. We will skip the details of discretization but a similar stability analysis can be performed for a discrete time system as well.
</p>

<h3>Introducing Recurrent Neural Network</h3>
<p>
The model represented by Equation <a href="#eq1">(1)</a> is a special form of recurrent neural network (RNN). A general recurrent neural network is fully nonlinear and do not have nice a state-space form. Nonlinearities are also important as they are needed for the modeling fidelity. The nonlinear parts are added as projection layers before and after the linear state-space components as will see later. The recurrent neural network offers a significant advantage over Transformers when it comes to the inference speed. Have a look at Equation <a href="#directy">(3)</a>. What do you see? The output can easily be computed in constant time using the value of current state $$\mathbf{X}$$. Transformers require $$\mathcal{O}(L^2)$$ time to compute the output where $$L$$ is the length of the sequence.
Furthermore, the Transformers cannot process sequences longer than the maximum limit defined by its architecture. The RNN can process sequences of any length including sequences longer than longest sequence it saw during the training.
</p>

<a class="marker" href="#difficultyrnn" id="difficultyrnn"></a>
<h4>The difficulty of training a nonlinear RNN</h4>
<details open style="margin-top:5px;padding:20px;">
  <summary>Summary of nonlinear RNN</summary>
  <p>
A nonlinear recurrent neural network however are difficult to train due to vanishing or exploding gradient problem. The dynamics are either too stable and unstable and the information signals either vanish or explode as we traverse across the sequence<d-cite bibtex-key="pascanu2013difficulty"></d-cite>.</p>
<d-figure>
  <img src="rnnbifurcation.png" style="height:200px;width:500px;display: block; margin-left: auto; margin-right: auto;margin-top:10px;"/>
  <figcaption style="text-align:center; margin-top:10px;">
    <a class="marker" href="#springmass">
      Figure 2: Bifurcation diagram for a single node RNN.
    </a>
  </figcaption>
</d-figure>
<p>
The figure above is taken from a classical paper listed as Ref <d-cite bibtex-key="pascanu2013difficulty"></d-cite>. These instabilities arise in nonlinear systems due to phenomenon known as bifurcation where
there is discontinuous jump in asymptotic state due to very small changes in the parameter.
Figure 2 shows the asymptotic values of neuron state $$x$$ in a single neuron RNN as 
we vary the bias parameter $$b$$. The figure shows 3 distinct parameter regimes: 1) $$ b < b_2 $$ 2) $$ b_2 < b < b_1 $$ 3) $$ b > b_1 $$. There is only one asymptotic state which varies smoothly as $$b$$ changes for the regime 1 with $$ b < b_2 $$. However, the regime 2 with $$ b_2 < b < b_1 $$ is very interesting. There are 3 possible asymptotic states in this regime with 2 of them being stable while the third one shown as dashed line is unstable. The neuron can end up in any of these states depending on the initial condition or the amount of perturbation from its current state. There is a <em>basin of attraction</em> for each of the 2 stable state. What does this mean in terms of training the RNN? It means the RNN could end up loosing all its learning as $$ b $$ crosses the value $$ b_2 $$ because the RNN may jump from one stable state to another stable state. The regime 3 is even more catastrophic as the the RNN will surely undergo a discontinuous jump as $$ b $$ crosses the value of $$ b_1 $$ and as result forget all its prior learnings. This is an example of subcritical pitchfork bifurcation. 
</p>
<p>Thus, the RNNs do not perform as well as Transformers that densely route information over the token graph using the self-attention mechanism. They cannot model long sequences like a DNA. Transformers however have a significant disadvantage due to their quadratic complexity and once again cannot be used to model long sequences.</p>
</details>

<p style="margin-top:10px;">
  <em>
  This sets the motivation for this article. We want to design a neural network that scales as $$\mathcal{O}(L)$$ during training and can do inference in constant time where $$L$$ is the length of sequence and $$L$$ can be very long up to order of 1 million tokens. We want deep information propagation capability that can transmit the signal for very long sequence lengths.
  </em>
</p>

<a class="marker" href="#koopmanoperator" id="koopmanoperator"></a>
<h3>Koopman operator: Linearized dynamics in a nonlinear function space</h3>
<p>
We saw that state-space models represented as linear system allows us to explicitly design for its stability characteristics by placing the real part of eigenvalues of the state transition matrix $$A$$ in the left half of the complex plane. Furthermore, we will see later that a diagonalized version $$A$$ can also help parallelize computations using parallel scans. Clearly, its advantageuous to have the dynamics part in a linear form.
We can separate the nonlinear part of the network that imparts modeling fidelity from the linearized dynamics using the Koopman operator theory<d-cite bibtex-key="koopman1932dynamical"></d-cite>. The Koopman operator theory says that it is possible to find an appropriate coordinate transform where the strongly nonlinear dynamics is approximately linear<d-cite bibtex-key="lusch2018deep"></d-cite>. Stated simply, we can represent these nonlinear transforms using a feedforward multi-layer perceptron (MLP) and apply the methods of linear state-space system to the output. The nonlinear transform and the linearized dynamics is learned end-to-end using deep learning.<aside>The idea is similar to kernel functions in Support Vector Machines which finds linearly separable inner product space using Kernel functions. The Gaussian kernel projects the input into an infinte dimensional space similar to the Koopman eigenfunctions.</aside>
</p>

<a class="marker" href="#designingrnn" id="designingrnn"></a>
<h3>Designing the linear recurrent block</h3>
<p>
Let us now to get to the core of this article. The core unit that processess the sequence will be called
the Linear Recurrent Unit (LRU). Most of the presentation in this section is borrowed from Ref <d-cite bibtex-key="orvieto2023resurrecting"></d-cite>. There are other work on structured state space models that builds upon the same idea of interleaving nonlinear MLP blocks with linear state-space models and differ in the way they structure and initialize $$A$$ and $$B$$ matrices. These include S4<d-cite bibtex-key="gu2021efficiently"></d-cite>, S5<d-cite bibtex-key="smith2022simplified"></d-cite>, and, Mamba<d-cite bibtex-key="gu2023mamba"></d-cite>.
</p>

<p>Lets once again rewrite the linear recurrence relationship in discrete and from here on we will use $$u_k$$ to represent the discrete input at time step $$k$$
<d-math block>
  x_{k} = Ax_{k-1} + Bu_k
</d-math>
The above recurrence relationship can easily be unrolled as follows:
<d-math block>
  x_0 = Bu_0, \quad x_1 = ABu_0 + Bu_1, \quad x_2 = A^2Bu_0 + ABu_1 + B u_2,
</d-math>
<a class="marker" href="#scanrecurrence" id="scanrecurrence"></a>
<d-math block>
  \implies x_k = \sum_{j=0}^{k-1} A^jBu_{k-j} \qquad (4)
</d-math>
</p>

<a class="marker" href="#diagonilizinga" id="diagonilizinga"></a>
<h4>Diagonalizing $$A$$</h4>
<p>
Note that if $$A$$ in above relationship is diagonal then $$A^j$$ can be easily computed. Furthermore, linear RNN layers with a diagonal structure enable efficient parallel unrolling of the recurrent process through parallel scans, leading to significantly faster training speeds<d-cite bibtex-key="martin2017parallelizing"></d-cite>. We can make use of the fact that every non-diagonal matrix is diagonalizable<d-cite bibtex-key="axler1997linear"></d-cite> to apply the eigenvectors based coordinate transform to diagnonalize $$A$$ resulting in a diagonal matrix with potentially complex valued entries. Thus, the Equation <a href="#scanrecurrence">(4)</a> can be expressed as follows:
<a class="marker" href="#diagrecurrence" id="diagrecurrence"></a>
<d-math block>
  x_k = \sum_{j=0}^{k-1} \Lambda^j B u_{k-j} \qquad (5)
</d-math>
where $$\Lambda$$ is a diagonal matrix with eigenvalues of $$A$$ as its diagonal entries.
</p>
<p>

<a class="marker" href="#stableexponentialparametrization" id="stableexponentialparametrization"></a>
<h4>Stable exponential parameterization</h4>
<p>
It is easy to see that the norm of component $$j$$ of $$x$$ at timestamp $$k$$ evolves such that $$x_{k,j} := \mathcal{O}(\lambda_j^k)$$. We can use exponential parameterization of $$\lambda_j$$ discussed in the next section to bring it in same form as Equation <a href="#eq2">(2)</a> and express it as $$x_{k,j} := \mathcal{O}(e^{k(-\nu_j+i\theta_j)})$$.
</p>
<d-figure>
  <img src="stepforce.png" style="height:200px;width:550px;display: block; margin-left: auto; margin-right: auto;margin-top:10px;"/>
  <figcaption style="text-align:center; margin-top:10px;">
    <a class="marker" href="#twoforce">
      Figure 3: The input for the sequence $$aa$$. The sequence $$ab$$ will have the second pulse of opposite sign.
    </a>
  </figcaption>
</d-figure>
<d-figure>
  <img src="springmassdampersimulation.png" style="height:350px;width:550px;display: block; margin-left: auto; margin-right: auto;margin-top:10px;"/>
  <figcaption style="text-align:center; margin-top:10px;">
    <a class="marker" href="#springmassdynamics">
      Figure 4: The response of system with sequence input $$aa$$ for low damping, highly damped and negatively damped systems.  
    </a>
  </figcaption>
</d-figure>
<d-figure>
  <img src="springmassdampersimulation1.png" style="height:350px;width:550px;display: block; margin-left: auto; margin-right: auto;margin-top:10px;"/>
  <figcaption style="text-align:center; margin-top:10px;">
    <a class="marker" href="#springmassdynamics1">
      Figure 5: The response of system with sequence input $$ab$$ for low damping, highly damped and negatively damped systems. 
    </a>
  </figcaption>
</d-figure>
<p>
  Therefore, a sufficient condition for $$x_{k,j}$$ to not explode and ensure stability is $$\nu_j > 0$$ for all $$j$$. It is also important to note the if $$\nu_j$$ is very large then $$x_{k,j}$$ will <em>vanish</em> which is an impediment in modeling long sequences where $$k$$ is very large. The information propagation capacity and stability of dynamics in forward pass is demonstrated in Figures 4 and 5. Lets model two sequences each with 2 tokens. The first sequence is $$aa$$ and the second sequence is $$ab$$. $$a$$ and $$b$$ tokens have 1-dimensional embeddings of 1.0 and -1.0 respectively. These inputs are fed to the system at an interval of 10 seconds. The inputs for $$aa$$ is shown in Figure <a href="#stepforce">(3)</a>. The input for $$ab$$ will be similar with the opposite sign of the pulse at $$10^{th}$$ second. We consider the response of 3 different systems with -- 1) low damping ($$\nu_j > 0$$) 2) high damping ($$\nu_j \gg 0$$) 3) negative damping ($$\nu_j < 0$$) for these two sequences. The information for the first token $$a$$ dies down for the highly damped system and final state is indistinguishable for the two sequences. However, for an appropriately damped system shown in green the final state differs for the two sequences representing successful information propagation. The amplitude for the negative damped case will gradually grow unbounded for both the sequences making the network unstable. Thus, we need to set the right inductive bias in the model for it be appropriately damped in order to foster long range information propagation capability.<aside>The Glorot initialization naturally places the eigenvalues within the unit circle in the complex plane.</aside>
</p>
<p>
  We enforce the condition above making use of Lemma 3.2 in Ref <d-cite bibtex-key="orvieto2023resurrecting"></d-cite> which is given as follows:
  Let $$u_1,u_2$$ be independent uniform random variables on the interval $$[0,1]$$. Let $$0\le r_{\min}\le r_{\max}\le1$$. Compute $$\nu = -\frac{1}{2}\log\left(u_1(r_{\max}^2-r_{\min}^2)+r_{\min}^2\right)$$ and $$\theta = 2\pi u_2 $$. Then $$\exp(-\nu+i\theta)$$ is uniformly distributed on the ring in complex plane $$\mathbb{C}$$ between circles of radii $$r_{\min}$$ and $$r_{\max}$$.
  </p>
  <d-figure>
    <img src="exponentialinitialization.png" style="height:200px;width:250px;display: block; margin-left: auto; margin-right: auto;margin-top:10px;"/>
    <figcaption style="text-align:center;">
      <a class="marker" href="#exponentiali">
        Figure 6: Exponential initialization of eigenvalues within a ring.
      </a>
    </figcaption>
  </d-figure>
  <p>
  This is shown in the above figure with $$r_{min} = 0.4$$ and $$r_{max} = 0.9$$.
  This suggests a natural parameterization for $$A$$ as $$\Lambda = \text{diag}(\exp(-\nu + i \theta))$$ with $$\nu$$ and $$\theta$$ as the learnable parameters.
</p>
<p>
  Exponential parameterization only solves half of the problem by keeping the eigenvalues confined within
  a ring inside the unit circle and therefore bounded. We also need ensure the stability of the dynamics discussed in spring-mass-damper example by having a positive damping. We need to ensure that $$\nu > 0$$ (this ensures positive damping similar to $$c > 0$$ in the spring-mass-damper system).
  This is easy to do if use another positive nonlinearity in the form of exponential: $$\lambda_j:=\exp(-\exp(\nu_j^{\log})+i\theta_j)$$, where $$ \nu^{\log}_j $$ is the parameter that is optimized, and we set $$\nu_j^{\log} := \log(\nu_j)$$ where $$\nu_j > 0$$. We can see from Equation <a href="#diagrecurrence">(5)</a> having the term $$\Lambda^j$$ that we need to initialize the eigenvalue closer to unit disk by making $$r_{min}$$ closer to 1 ($$\lambda_j^k \approx 0$$ when $$\lambda_j \ll 1$$) for deep information propagation and model long range interactions. 
</p>
<a class="marker" href="#transientdynamics" id="transientdynamics"></a>
<h4>Designing the transient dynamics through small phase</h4>
<p>
  Additionally, in the studies in Ref <d-cite bibtex-key="orvieto2023resurrecting"></d-cite> it was also found that the LRU has to be initialized with a small phase $$\theta_j \approx [0,\pi/50]$$ to be able to do well on the most difficult task of Path-X.
<p>
<div style="display: flex; justify-content: space-around; align-items: center;">
  <d-figure>
      <img src="sample_128_positive.png" style="height:270px;width:270px;margin-top:10px;">
      <figcaption style="text-align:center; margin-top:10px;">
        Path-X positive sample.
      </figcaption>
  </d-figure>
  <d-figure>
      <img src="sample_128_negative.png" style="height:270px;width:270px;margin-top:10px;">
      <figcaption style="text-align:center; margin-top:10px;">
        Path-X negative sample.
      </figcaption>
  </d-figure>
</div>
<p>
The figure above shows the positive and negative samples for the Path-X task. The image consists of 16000 pixels which are fed sequentially to the model. The task is to ascertain if the two white dots are connected by a path. Unconstrained initialization between $$[0,2\pi]$$ can result in high frequency dynamics with large number of oscillations (see Figure 4 in Ref <d-cite bibtex-key="orvieto2023resurrecting"></d-cite>). <em>The presence of high frequency components make the system quickly settle into its autonomous modes and the memory of the input is lost. We would like the transient dynamics to be driven largely by the inputs instead of being inherent to the system (see Ref <d-cite bibtex-key="PhysRevLett.108.244101"></d-cite>).</em> 
</p>

<h4>The full neural network architecture</h4>
<d-figure>
  <img src="liru1.jpg" style="height:400px;width:700px;display: block; margin-left: auto; margin-right: auto;margin-top:10px;"/>
  <figcaption style="text-align:center; margin-top:10px;">
    <a class="marker" href="#linearrecurrentunitnetwork">
      Figure 7: The complete architecture for LRU. This is Figure 1 from Ref <d-cite bibtex-key="orvieto2023resurrecting"></d-cite>.
    </a>
  </figcaption>
</d-figure>
<p>
  The complete architecture for the entire network is shown in Figure <a href="#linearrecurrentunitnetwork">(7)</a>.
  The $$M$$ dimensional input of length $$L$$ is projected into a dimension $$H$$ (see Section <a href="#koopmanoperator">Koopman operator</a>). The layer normalization or batch normalization is applied before passing it on to LRU. The LRU outputs are further processed using GLU and skip connection.
</p>
<h3>Why use linear recurrent block?</h3>
<p>
  Given all the discussions so far, let us summarize as to why we want to use a linear recurrent block.
<ul>
  <li>Transformers are inefficient with $$\mathcal{O}(L^2)$$ complexity during training and inference.</li>
  <li>RNNs have $$\mathcal{O}(L)$$ complexity during training and can do inference in constant time.</li>
  <li>A nonlinear RNN is difficult to train due to bifurcations encountered during training (see Section <a href="#difficultyrnn">The difficulty of training RNN</a>).</li>
  <li>The nonlinear RNN requires unrolling one time-step at a time and therefore cannot be parallelized.</li>
  <li>The nonlinearities added before the linear recurrent blocks are sufficient to ensure modeling fidelity and afford universal computation according to Koopman operator theory (see Section <a href="#koopmanoperator">Koopman Operator</a>).</li>
  <li>The linear recurrence can be computed in one single step by recursively expressing the state over the time range as a function of input. It is possible to relate the final state to input sequence with a simple equation without the need to compute the intermediate state (see Equation <a href="#scanrecurrence">(4)</a> in Section <a href="#designingrnn">Designing Linear Recurrent Unit</a>).</li>
  <li>Every non-diagonal state space model can be diagonalized with a reparametrization with potentially complex entries. The diagonalized $$A$$ offer additional computational efficiency using parallel scans<d-cite bibtex-key="Martin2017ParallelizingLR"></d-cite> (see Section <a href="#diagonalizinga">Diagaonalizing $$A$$</a>).</li>
  <li>We can design the stability characteristics of the linear system to be appropriately stable and ensure stability during training giving us the ability to model long sequences with fast training and inference (see Section <a href="#stableexponentialparametrization">Stable exponential parametrization</a>).</li>
</ul>
</p>

<h3>Further thoughts: Resurrecting nonlinear recurrent units on hardware</h3>
<p>
  We saw in the Section <a href="#transientdynamics">Designing the transient dynamics</a> that it was necessary to keep the phase small (imaginary part of the eigenvalue) to solve the most difficult task of Path-X in the long range arena benchmark. Thus, the information and memory about the sequence is stored in the transient dynamics
  of the linear dynamical system that we designed. We designed these transients to be stable by creating an inductive bias that places the real part of eigenvalue in the left of the complex plane.
  The transients will blow up if the system is unstable (see Figure <a href="#springmassdynamics">(4)</a> and <a href="#springmassdynamics1">(5)</a>). The linear state space models also afforded fast computation through the use of Equation <a href="#scanrecurrence">(4)</a> and parallel scans. I would like to bring to attention existence of nonlinear dynamical systems that are both unstable and stable at the same time. They are composed of multiple modes many of which are unstable. However, their oscillations are still bounded due to presence of nonlinearities. They have tremendous information processing capacity because they are repository of rich dynamical patterns in the form of transients that can serve as effective kernel.
  </p> 
  <d-figure>
    <img src="photonicnonlineartransientcomputing.png" style="height:150px;width:550px;display: block; margin-left: auto; margin-right: auto;margin-top:10px;"/>
  </d-figure>
  <p>
  They can be being implemented on an extremely high bandwidth specialized hardwares that are fast and energy efficient<d-cite bibtex-key="brunner2013parallel"></d-cite>.
  A photonics based hardware can process million words per second<d-cite bibtex-key="PhysRevX.7.011015"></d-cite>. There is also no need to train these nonlinear recurrent units on hardware. The only trainable component would be preprocessing and post processing components placed before these nonlinear recurrent unit blocks. 
</p>
</d-article>

<d-appendix>
  <d-bibliography src="bibliography.bib"></d-bibliography>
</d-appendix>
  
</body>