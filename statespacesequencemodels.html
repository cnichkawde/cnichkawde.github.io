<!doctype html>

<head>
  <script src="template.v2.js"></script>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf8">
  <title>Beyond Transformers: Structured State Space Sequence Models</title>
  <style id="distill-article-specific-styles">
      .subgrid {
    grid-column: screen; 
    display: grid; 
    grid-template-columns: inherit;
    grid-template-rows: inherit;
    grid-column-gap: inherit;
    grid-row-gap: inherit;
  }

  d-front-matter {
    display: none;
  }
  
  d-figure.base-grid {
    grid-column: screen;
    background: hsl(0, 0%, 97%);
    padding: 20px 0;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  d-figure {
    margin-bottom: 1em;
    position: relative;
  }
  
  d-figure > figure {
    margin-top: 0;
    margin-bottom: 0;
  }
  
  .shaded-figure {
    background-color: hsl(0, 0%, 97%);
    border-top: 1px solid hsla(0, 0%, 0%, 0.1);
    border-bottom: 1px solid hsla(0, 0%, 0%, 0.1);
    padding: 30px 0;
  }
  
  .pointer {
    position: absolute;
    width: 26px;
    height: 26px;
    top: 26px;
    left: -48px;
  }
  
  div#observablehq {
    font-family: inherit;
    font-size: inherit;
  }
  
  button {
    font-family : inherit;
    font-size: 1em;
  }

  
  /* Alignment of KaTeX. */
  #gnn-models span.katex-display {
    margin: 0.5em 0 0.5em 0em;
  }
  
  @media (max-width: 1000px) {
    d-contents {
      justify-self: start;
      align-self: start;
      grid-column-start: 2;
      grid-column-end: 6;
      padding-bottom: 0.5em;
      margin-bottom: 1em;
      padding-left: 0.25em;
      border-bottom: 1px solid rgba(0, 0, 0, 0.1);
      border-bottom-width: 1px;
      border-bottom-style: solid;
      border-bottom-color: rgba(0, 0, 0, 0.1);
    }
  }
  
  @media (min-width: 1000px) {
    d-contents {
      align-self: start;
      grid-column-start: 1;
      grid-column-end: 4;
      justify-self: end;
      padding-right: 3em;
      padding-left: 2em;
      border-right: 1px solid rgba(0, 0, 0, 0.1);
      border-right-width: 1px;
      border-right-style: solid;
      border-right-color: rgba(0, 0, 0, 0.1);
    }
  }
  
  @media (min-width: 1180px) {
    d-contents {
      grid-column-start: 1;
      grid-column-end: 4;
      justify-self: end;
      padding-right: 3em;
      padding-left: 2em;
      border-right: 1px solid rgba(0, 0, 0, 0.1);
      border-right-width: 1px;
      border-right-style: solid;
      border-right-color: rgba(0, 0, 0, 0.1);
    }
  }
  
  d-contents nav h3 {
    margin-top: 0;
    margin-bottom: 1em;
  }
  
  d-contents nav a {
    color: rgba(0, 0, 0, 0.8);
    border-bottom: none;
    text-decoration: none;
  }
  
  d-contents li {
    list-style-type: none;
  }
  
  d-contents ul {
    padding-left: 1em;
  }
  
  d-contents nav ul li {
    margin-bottom: 0.25em;
  }
  
  d-contents nav a:hover {
    text-decoration: underline solid rgba(0, 0, 0, 0.6);
  }
  
  d-contents nav ul {
    margin-top: 0;
    margin-bottom: 6px;
  }
  
  d-contents nav > div {
    display: block;
    outline: none;
    margin-bottom: 0.5em;
  }
  
  d-contents nav > div > a {
    font-size: 13px;
    font-weight: 600;
  }
  
  d-contents nav > div > a:hover, d-contents nav > ul > li > a:hover {
    text-decoration: none;
  }
  
  input[type="radio"] {
    vertical-align: unset !important;
  }
  
  .math-details {
    padding-left: 1em;
    padding-right: 1em;
    padding-bottom: 0em;
    padding-top: 1em;
    margin-bottom: 1em;
  }
  </style>

  <style>
    /* Style the details element */
    details {
      background-color: #e6f7ff;; /* Light grey background */
      border: 2px solid #ddd; /* Optional: Add a border */
      padding: 20px;
      padding-top: 0;
      margin-top: 0;
      margin-bottom: 0;
    }
  
     /* Hide the summary element */
    summary {
      display: none; /* Hide the element */
    }
  
    /* Expand the content by default */
    details[open] > :not(summary) {
      display: block;
    }

    h1 {
      line-height: 1.2;
      color: #333;
    }

    .author {
        font-size: 1.5em;
        font-family: "Rubik", sans-serif;
    }

    .published-date {
        font-size: 0.9em;
        color: #777;
    }
  </style>
</head>

<body>
  <!-- <d-front-matter>
    <script id='distill-front-matter' type="text/json">{
    "title": "Beyond Transformers: Structured State Space Sequence Models",
    "description": "This article describes SSSSM",
    "published": "Jan 10, 2017",
    "authors": [
      {
        "author":"Chetan Nichkawde",
        "authorURL":"https://linkedin.com/in/cnichkawde"
      }
    ],
    "katex": {
      "delimiters": [
        {"left": "$$", "right": "$$", "display": false}
      ]
    }
  }</script>
  </d-front-matter>
   -->
  
  <d-article>
    <header>
      <center>
        <h1>
          Beyond Transformers: <br> Structured State Space Sequence Models
        </h1>
      </center>
      <center>
        <p><span class="author">Chetan Nichkawde</span></p>
      </center>
      <center>
        <p><time class="published-date" datetime="2024-01-22">January 22, 2024</time></p>
      </center>
    </header>
    
    <a class="marker" href="#abstract" id="abstract"></a>
    <p>
      Sequence modeling has taken centre stage in the world of artificial intelligence with
      the advent of large language models. We have learned that if we throw enough computing and
      large amounts of data then a form of intelligence spontaneously emerges by doing something as simple as
      autoregressive pretraining. However, the cost of training these models and subsequent cost of inference
      makes it prohibitively difficult for large scale adoption. These models are built upon Transformers-based architectures. A new paradigm is rapidly evolving within the realm of sequence modeling that presents a marked advancement over the Transformer architectures. This new approach demonstrates superior capability compared to Transformers in accurately modeling extensive sequence lengths and contextual dependencies. It exhibits an order of magnitude improvement in computational efficiency during training and inference.
    </p>

    <a class="marker" href="#introduction" id="introduction"></a>
    <p></p>
  <details open>
  <summary><strong>Why do we want to go beyond Transformers?</strong></summary>
    <p>
    <h5>What makes Transformers great?</h5>
      The efficacy of self-attention due to its ability to route information densely within a context window, allowing it to model complex data.
    <h5>What are the challenges with Transformer models?</h5>
    <ul>
      <li>Inability to model anything outside of the context window.</li>
      <li>Quadratic complexity and scaling for training and inference with respect to the sequence length.</li>
    </ul>

  <h5>What makes State Space models great?</h5>
<ul>
<li>It has linear complexity with respect to the sequence length during the training time 
  which can easily be parallelized using parallel scans.</li>
<li>Many times faster during inference. It can do inference in constant time using the current state compared to Transformers 
  which requires computation of self-attention over the entire context.</li>
  <li>They can easily model very long range context and have 
    been shown to model a context length of about 1 million tokens.
    The Transformers cannot model long context due to quadratic scaling
  with respect to the sequence length.</li>
  <li>They can do inferences on sequences of any length and longer than the longest sequence in the training data.</li>
</ul>
</p>
</details>
 
<p style="margin-top:20px;">
The table below shows the results on Long Range Arena<d-cite bibtex-key="tay2020long"></d-cite> benchmark which has a set of tasks on data with long sequences.
</p>
<table style="width:100%; border-collapse: collapse;">
  <caption>
      <strong>Long Range Arena Benchmark</strong>
  </caption>
  <thead>
      <tr>
          <th>Model</th>
          <th>ListOps</th>
          <th>Text</th>
          <th>Retrieval</th>
          <th>sCIFAR</th>
          <th>Pathfinder</th>
          <th>Path-X</th>
          <th>Avg</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Transformer</td>
          <td>36.37</td>
          <td>64.27</td>
          <td>57.46</td>
          <td>42.44</td>
          <td>71.40</td>
          <td>&#10005;</td>
          <td>53.66</td>
      </tr>
      <tr>
          <td>Reformer</td>
          <td>37.27</td>
          <td>56.10</td>
          <td>53.40</td>
          <td>38.07</td>
          <td>68.50</td>
          <td>&#10005;</td>
          <td>50.56</td>
      </tr>
      <tr>
          <td>BigBird</td>
          <td>36.05</td>
          <td>64.02</td>
          <td>59.29</td>
          <td>40.83</td>
          <td>74.87</td>
          <td>&#10005;</td>
          <td>54.17</td>
      </tr>
      <tr>
          <td>Linear Trans.</td>
          <td>16.13</td>
          <td>65.90</td>
          <td>53.09</td>
          <td>42.34</td>
          <td>75.30</td>
          <td>&#10005;</td>
          <td>50.46</td>
      </tr>
      <tr>
          <td>Performer</td>
          <td>18.01</td>
          <td>65.40</td>
          <td>53.82</td>
          <td>42.77</td>
          <td>77.05</td>
          <td>&#10005;</td>
          <td>51.18</td>
      </tr>
      <tr>
          <td>FNet</td>
          <td>35.33</td>
          <td>65.11</td>
          <td>59.61</td>
          <td>38.67</td>
          <td>77.80</td>
          <td>&#10005;</td>
          <td>54.42</td>
      </tr>
      <tr>
          <td>Nystromformer</td>
          <td>37.15</td>
          <td>65.52</td>
          <td>79.56</td>
          <td>41.58</td>
          <td>70.94</td>
          <td>&#10005;</td>
          <td>57.46</td>
      </tr>
      <tr>
          <td>Luna-256</td>
          <td>37.25</td>
          <td>64.57</td>
          <td>79.29</td>
          <td>47.38</td>
          <td>77.72</td>
          <td>&#10005;</td>
          <td>59.37</td>
      </tr>
      <!-- <tr>
          <td><strong>S4</strong></td>
          <td><strong>59.60</strong></td>
          <td><strong>86.82</strong></td>
          <td><strong>90.90</strong></td>
          <td><strong>88.65</strong></td>
          <td><strong>94.20</strong></td>
          <td><strong>96.35</strong></td>
          <td><strong>86.09</strong></td>
      </tr> -->
      <tr>
        <td><strong>LRU</strong></td>
        <td><strong>60.2</strong></td>
        <td><strong>89.4</strong></td>
        <td><strong>89.9</strong></td>
        <td><strong>89.0</strong></td>
        <td><strong>95.1</strong></td>
        <td><strong>94.2</strong></td>
        <td><strong>86.3</strong></td>
    </tr>
  </tbody>
</table>
<p>
We can see that the results for Linear Recurrent Unit (LRU) discussed in this article far exceed all the Transformers based models. Furthermore, the state space based models are 5 to 10 times faster on inference speed while requiring only 10% of the memory compared to Transformers. The longest context is about 16000 for the task Path-X. All of the Transformer based models are not able to solve this task while LRU has an accuracy of about 94%.
<em>
The purpose of this article is to develop a foundational level understanding on how structured state space sequence models work in efficiently modeling long sequences.
</em>
</p>

<a class="marker" href="#classicalspringmassdampersystem" id="classicalspringmassdampersystem"></a>
<h3>The quintessential spring-mass-damper system</h3>
<p>
We start our discussion with spring-mass-damper system which is one of the most widely studied systems in classical mechanics and dynamics. The ideas from this template system have already been applied to various fields in science and engineering. For instance, if you want to understand the flight stability characteristics of a rocket flying in the atmosphere carrying a large amount of liquid fuel, you can apply a similar analysis performed for the rocket system linearized around an equilibrium point<d-cite bibtex-key="nichkawde2004stability"></d-cite>. <em>We will see later that the linear dynamical systems also form the backbone of the futuristic sequence models
which alleviates the challenges with Transformer models while offering the same level of modeling fidelity</em>.
</p>

<d-figure>
  <img src="https://www.shimrestackor.com/Physics/Spring_Mass_Damper/Figs/2-cart.png" style="height:200px;width:500px;display: block; margin-left: auto; margin-right: auto;margin-top:10px;"/>
  <figcaption style="text-align:center; margin-top:10px;">
    <a class="marker" href="#springmass">
      Figure 1: A spring mass damper system with external forcing.
    </a>
  </figcaption>
</d-figure>

<p>
Shown above is an object with mass $$m$$ attached to a wall with spring with a spring constant $$k$$ and a damper with damping coefficient $$c$$. The displacement of the object is represented by $$x(t)$$. The object is forced by an external time dependent force $$f(t)$$. The $$f(t)$$ we will later see is equivalent to our sequential data. The spring $$k$$ is a potential energy store that is converted into kinetic energy of the object in an oscillatory fashion. The damper $$c$$ usually dissipates the energy when $$c$$ is positive. A strong damping can quickly dissipate the energy leading the object coming to almost rest (remember vanishing gradients?!). However, $$c$$ can also be negative in which case the damper can actually absorb more energy from the environment and add it to the system eventually leading to explosion (remember exploding gradients?!). We will see later these dynamic stability characteristics have a deep connection with the modeling fidelity of our sequence model and we want to create inductive biases in the model formulation such that a similar linear dynamical system like the above has good dynamic characteristics that facilitate learning.
<aside>The vanishing and exploding gradients can also be understood in terms of the stability of the dynamics of backpropagation. Here we are motivated by the need to find efficient parametrization to ensure just the right kind of dynamics during the forward pass.</aside>
</p>

<p>
Let us formulate the equations of motion for this system using the Newton's second law of motion:
<d-math block>F = ma</d-math>
where the force $$F$$ is
<d-math block> F = -c\dot{x}-kx</d-math>
The spring-mass-damper system also has a time-dependent driving force $$f(t)$$ which represents the input to the system and our sequential data.
Putting it all together:
<d-math block>m\ddot{x} = -c\dot{x}-kx+f(t)</d-math>
Now we express this equation in state-space form.
Let $$x_1 = x$$ and $$x_2 = \dot{x}$$.
Rewriting above:
<d-math block>
\begin{array}{l}
\dot{x}_1 = x_2 \\
\dot{x}_2 = -\frac{c}{m}x_2 - \frac{k}{m}x_1 + \frac{1}{m}f(t)
\end{array}
</d-math>
Expressing it in matrix form:
<a class="marker" href="#eq1" id="eq1"></a>
<d-math block>\dot{\mathbf{X}} = A\mathbf{X} + B\mathbf{U}(t) \qquad (1)
</d-math>
where
<d-math block>
\mathbf{X} =
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}, \quad \mathbf{U} =
\begin{bmatrix}
0 \\
f(t)
\end{bmatrix}
</d-math>
<d-math block>
A =
\begin{bmatrix}
0 & 1 \\
-\frac{k}{m} & -\frac{c}{m}
\end{bmatrix}, \quad
B =  \begin{bmatrix}
0 \\
\frac{1}{m}
\end{bmatrix}
</d-math>
</p>
<p>
  <em>
    The dynamical stability characteristics of any system expressed in the above form, also known as state-space form, can be completely understood by inspecting the eigenvalues of the matrix $$A$$.
  </em>
    We want to have a positive damping to ensure stability. We can easily extend the above analysis to a multi-dimensional system.
    <aside>We want the system to be marginally damped but not overdamped from a deep learning perspective.</aside>
</p>
<p>    
Let's work out the formula for eigenvalues. The characteristic equation is given by:
<d-math block>
m\lambda^2 + c\lambda + k = 0
</d-math>
The roots (eigenvalues) are given by:
<d-math block>
\lambda_{1,2} = \frac{-c \pm \sqrt{c^2 - 4mk}}{2m}
</d-math>
When the discriminant $$\Delta = c^2 - 4mk$$ is negative, the eigenvalues become complex conjugates.
Let's consider the case where the discriminant is negative:
The natural frequency $$\omega_n$$ of the system defined as:
<d-math block>
\omega_n = \sqrt{\frac{k}{m}}
</d-math>
The eigenvalues can then be expressed in terms of $$\omega_n$$ as:
<d-math block>
\lambda_{1,2} = \frac{-c}{2m} \pm j\omega_d
</d-math>
where $$j$$ is the imaginary unit, and $$\omega_d$$ is the damped natural frequency given by:
<d-math block>
\omega_d = \sqrt{\omega_n^2 - \frac{c^2}{4m^2}}
</d-math>
</p>
<p>
  It is always possible to apply a coordinate transform such that the state transition matrix $$A$$ is diagonalized with the eigenvalues as the diagonal entries<d-cite bibtex-key="axler1997linear"></d-cite>.
  Thus, we can get a decoupled set of equations of the following form:
  <d-math block>
    \frac{dx(t)}{dt} = \lambda x(t)
  </d-math>
  where $$\lambda$$ is the eigenvalue.
  To solve this ODE, we can separate variables and integrate:
  <d-math block>
    \int \frac{dx}{x} = \lambda \int dt
  </d-math>
  Integrating both sides gives:
  <d-math block>
    \ln(x) = \lambda t + C
  </d-math>
  where $$C$$ is the constant of integration.
  Taking the exponential of both sides:
  <d-math block>
    x(t) = e^{\lambda t + C}
  </d-math>
  <d-math block>
    x(t) = e^{C} e^{\lambda t}
  </d-math>
  We can rewrite the above as:
  <a class="marker" href="#eq2" id="eq2"></a>
  <d-math block>
    x(t) = x_0e^{\lambda t} \qquad (2)
  </d-math>
  where $$x_0$$ is the initial state.  
</p>
<p>
Since the real part of $$\lambda$$ is $$\frac{-c}{2m}$$, it is easy to see from Equation (2) why we need a positive damping $$c$$ for the system to be stable. The exponential term will blow up as time progresses for a negative value of $$c$$ making the system unstable. We will revisit this equation in the Section <a href="#designingrnn">Designing linear recurrent unit.</a>
</p>
<p>
It is important to note that the system represented by Equation <a href="#eq1">(1)</a> is linear. The matrices $$A$$ and $$B$$ do not depend on the state $$x$$ and the input $$f(t)$$. At this point, it would be good to introduce another equation for the output $$y$$.
<a class="marker" href="#directy" id="directy"></a>
<d-math block>
  y(t) = C\mathbf{X} \qquad (3)
</d-math>
The output for the autoregressive model is the future value in time of $$f(t+1)$$. It would be the next token $$u_{k+1}$$ for a sequence model.
</p>

<p>
The data in the real world is always discrete especially if you are trying to model discrete sequences like language, protein or DNA. The continuous time ordinary differential Equation <a href="#eq1">(1)</a> can easily be discretized using methods like Zero-Order Hold or Bilinear transform. We will skip the details of discretization but a similar stability analysis can be performed for a discrete time system as well.
</p>

<h3>Introducing Recurrent Neural Network</h3>
<p>
The model represented by Equation <a href="#eq1">(1)</a> is a special form of recurrent neural network (RNN). A general recurrent neural network is fully nonlinear and does not have nice a state-space form. Nonlinearities are also important as they are needed for the modeling fidelity. The nonlinear parts are added as projection layers before and after the linear state-space components as will see later. The recurrent neural network offers a significant advantage over Transformers when it comes to the inference speed. Have a look at Equation <a href="#directy">(3)</a>. What do you see? The output can easily be computed in constant time using the value of current state $$\mathbf{X}$$. Transformers require $$\mathcal{O}(L^2)$$ time to compute the output where $$L$$ is the length of the sequence.
Furthermore, the Transformers cannot process sequences longer than the maximum limit defined by its architecture. The RNN can process sequences of any length including sequences longer than the longest sequence it saw during the training.
</p>

<a class="marker" href="#difficultyrnn" id="difficultyrnn"></a>
<h4>The difficulty of training a nonlinear RNN</h4>
<p>
A nonlinear recurrent neural network however is difficult to train due to vanishing or exploding gradient problem. The dynamics are either too stable or unstable and the information signals either vanish or explode as we traverse across the sequence. Such instabilities can be understood from the linearized version of the nonlinear RNN and using an analysis similar to the discussion in the Section <a href="#classicalspringmassdampersystem">The spring mass damper system</a><d-cite bibtex-key="pascanu2013difficulty"></d-cite>.
</p>
<d-figure>
  <img src="rnnbifurcation.png" style="height:200px;width:500px;display: block; margin-left: auto; margin-right: auto;margin-top:10px;"/>
  <figcaption style="text-align:center; margin-top:10px;">
    <a class="marker" href="#springmass">
      Figure 2: Bifurcation diagram for a single node RNN.
    </a>
  </figcaption>
</d-figure>
<p>
Additionally, these instabilities can also arise in nonlinear systems due to a phenomenon known as bifurcation where
there is a discontinuous jump in the asymptotic state due to very small changes in the parameter. The figure above is taken from a classical paper listed as Ref <d-cite bibtex-key="pascanu2013difficulty"></d-cite>. 
Figure 2 shows the asymptotic values of neuron state $$x$$ in a single neuron RNN as 
we vary the bias parameter $$b$$. The figure shows 3 distinct parameter regimes: 1) $$ b < b_2 $$ 2) $$ b_2 < b < b_1 $$ 3) $$ b > b_1 $$. There is only one asymptotic state which varies smoothly as $$b$$ changes for the regime 1 with $$ b < b_2 $$. However, the regime 2 with $$ b_2 < b < b_1 $$ is very interesting. There are 3 possible asymptotic states in this regime with 2 of them being stable while the third one shown as dashed line is unstable. The neuron can end up in any of these states depending on the initial condition or the amount of perturbation from its current state. There is a <em>basin of attraction</em> for each of the 2 stable state. What does this mean in terms of training the RNN? It means the RNN could end up losing all its learning as $$ b $$ crosses the value $$ b_2 $$ because the RNN may jump from one stable state to another stable state. The regime 3 is even more catastrophic as the the RNN will surely undergo a discontinuous jump as $$ b $$ crosses the value of $$ b_1 $$ and as a result forget all its prior learnings. This is an example of subcritical Hopf bifurcation. 
</p>
<p>
  Thus, the nonlinear RNNs do not perform as well as Transformers that densely route information over the token graph using the self-attention mechanism. They cannot model long sequences like a DNA. Transformers however have a significant disadvantage due to their quadratic complexity and once again cannot be used to model long sequences.</p>

<p style="margin-top:10px;">
  <em>
  This sets the motivation for this article. We want to design a neural network that scales as $$\mathcal{O}(L)$$ during training and can do inference in constant time where $$L$$ is the length of the sequence and $$L$$ can be very long up to the order of 1 million tokens. We want deep information propagation capability that can transmit the signal for very long sequence lengths.
  </em>
</p>

<a class="marker" href="#koopmanoperator" id="koopmanoperator"></a>
<h3>Koopman operator: Linearized dynamics in a nonlinear function space</h3>
<p>
We saw that state-space models represented as linear system allow us to explicitly design for its stability characteristics by placing the real part of eigenvalues of the state transition matrix $$A$$ in the left half of the complex plane. Furthermore, we will see later that a diagonalized version $$A$$ can also help parallelize computations using parallel scans. Clearly, it's advantageous to have the dynamics part in a linear form.
We can separate the nonlinear part of the network that imparts modeling fidelity from the linearized dynamics using the Koopman operator theory<d-cite bibtex-key="koopman1932dynamical"></d-cite>. The Koopman operator theory says that it is possible to find an appropriate coordinate transform where the strongly nonlinear dynamics is approximately linear<d-cite bibtex-key="lusch2018deep"></d-cite>. Stated simply, we can represent these nonlinear transforms using a feedforward multi-layer perceptron (MLP) and apply the methods of linear state-space system to the output. The nonlinear transform and the linearized dynamics are learned end-to-end using deep learning.<aside>The idea is similar to kernel functions in Support Vector Machines which finds linearly separable inner product space using Kernel functions. The Gaussian kernel projects the input into an infinite dimensional space similar to the Koopman eigenfunctions.</aside>
</p>

<a class="marker" href="#designingrnn" id="designingrnn"></a>
<h3>Designing the linear recurrent block</h3>
<p>
Let us now get to the core of this article. The core unit that processes the sequence will be called
the Linear Recurrent Unit (LRU). Most of the presentation in this section builds upon the work from Ref <d-cite bibtex-key="orvieto2023resurrecting"></d-cite>. There are other work on structured state space models that develops the same idea of interleaving nonlinear MLP blocks with linear state-space models and differ in the way they structure and initialize $$A$$ and $$B$$ matrices. These include S4<d-cite bibtex-key="gu2021efficiently"></d-cite>, S5<d-cite bibtex-key="smith2022simplified"></d-cite>, and, Mamba<d-cite bibtex-key="gu2023mamba"></d-cite>.
</p>

<p>Lets once again rewrite the linear recurrence relationship in discrete form and from here on we will use $$x_k$$ to represent the value of state and $$u_k$$ to represent the discrete input at time step $$k$$
<d-math block>
  x_{k} = Ax_{k-1} + Bu_k
</d-math>
The above recurrence relationship can easily be unrolled as follows:
<d-math block>
  x_0 = Bu_0, \quad x_1 = ABu_0 + Bu_1, \quad x_2 = A^2Bu_0 + ABu_1 + B u_2,
</d-math>
<a class="marker" href="#scanrecurrence" id="scanrecurrence"></a>
<d-math block>
  \implies x_k = \sum_{j=0}^{k-1} A^jBu_{k-j} \qquad (4)
</d-math>
</p>

<a class="marker" href="#diagonilizinga" id="diagonilizinga"></a>
<h4>Diagonalizing $$A$$</h4>
<p>
Note that if $$A$$ in above relationship is diagonal then $$A^j$$ can be easily computed. Furthermore, linear RNN layers with a diagonal structure enable efficient parallel unrolling of the recurrent process through parallel scans, leading to significantly faster training speeds<d-cite bibtex-key="martin2017parallelizing"></d-cite>. We can make use of the fact that every non-diagonal matrix is diagonalizable<d-cite bibtex-key="axler1997linear"></d-cite> to apply the eigenvectors based coordinate transform to diagonalize $$A$$ resulting in a diagonal matrix with potentially complex valued entries. Thus, the Equation <a href="#scanrecurrence">(4)</a> can be expressed as follows:
<a class="marker" href="#diagrecurrence" id="diagrecurrence"></a>
<d-math block>
  x_k = \sum_{j=0}^{k-1} \Lambda^j B u_{k-j} \qquad (5)
</d-math>
where $$\Lambda$$ is a diagonal matrix with eigenvalues of $$A$$ as its diagonal entries.
</p>
<p>

<a class="marker" href="#stableexponentialparametrization" id="stableexponentialparametrization"></a>
<h4>Stable exponential parameterization</h4>
<p>
It is easy to see that the norm of component $$j$$ of $$x$$ at timestamp $$k$$ evolves such that $$x_{k,j} := \mathcal{O}(\lambda_j^k)$$ where $$\lambda_j$$ is the diagonal entry in the $$j^{th}$$ row of the diagonal matrix $$\Lambda$$ discussed in the Equation <a href="#diagrecurrence">(5)</a> and it is the $$j^{th}$$ eigenvalue of $$A$$. We can use exponential parameterization of $$\lambda_j = e^{\nu_j+i\theta_j}$$ discussed in the next section to bring it in same form as Equation <a href="#eq2">(2)</a> and thus $$x_{k,j} := \mathcal{O}(e^{-k\nu_j}e^{i\theta_j})$$. Therefore, a sufficient condition for $$x_{k,j}$$ to not explode and ensure stability is $$\nu_j > 0$$ for all $$j$$.
</p>
<d-figure>
  <img src="stepforce.png" style="height:200px;width:550px;display: block; margin-left: auto; margin-right: auto;margin-top:10px;"/>
  <figcaption style="text-align:center; margin-top:10px;">
    <a class="marker" href="#twoforce">
      Figure 3: The input for the sequence $$aa$$. The sequence $$ab$$ will have the second pulse of the opposite sign.
    </a>
  </figcaption>
</d-figure>
<d-figure>
  <img src="springmassdampersimulation.png" style="height:350px;width:550px;display: block; margin-left: auto; margin-right: auto;margin-top:10px;"/>
  <figcaption style="text-align:center; margin-top:10px;">
    <a class="marker" href="#springmassdynamics">
      Figure 4: The response of the system with sequence input $$aa$$ for low damping, highly damped, and, negatively damped systems.  
    </a>
  </figcaption>
</d-figure>
<d-figure>
  <img src="springmassdampersimulation1.png" style="height:350px;width:550px;display: block; margin-left: auto; margin-right: auto;margin-top:10px;"/>
  <figcaption style="text-align:center; margin-top:10px;">
    <a class="marker" href="#springmassdynamics1">
      Figure 5: The response of the system with sequence input $$ab$$ for low damping, highly damped, and, negatively damped systems. 
    </a>
  </figcaption>
</d-figure>
<p>
  It is also important to note that if $$\nu_j$$ is very large then $$x_{k,j}$$ will <em>vanish</em> which is an impediment in modeling long sequences where $$k$$ is very large. The information propagation capacity and stability of dynamics in the forward pass is demonstrated in Figures <a href="#springmassdynamics">(4)</a> and <a href="#springmassdynamics1">(5)</a>. Let's model two sequences each with 2 tokens. The first sequence is $$aa$$ and the second sequence is $$ab$$. $$a$$ and $$b$$ tokens have 1-dimensional embeddings of 1.0 and -1.0 respectively. These inputs are fed to the system at an interval of 10 seconds. The inputs for $$aa$$ is shown in Figure <a href="#stepforce">(3)</a>. The input for $$ab$$ will be similar with the opposite sign of the pulse at $$10^{th}$$ second. We consider the response of 3 different systems with -- 1) low damping ($$\nu_j > 0$$) 2) high damping ($$\nu_j \gg 0$$) 3) negative damping ($$\nu_j < 0$$) for these two sequences. The information for the first token $$a$$ attenuates quickly for the highly damped system and the final state is indistinguishable for the two sequences. However, for an appropriately damped system shown in green the final state differs for the two sequences representing successful information propagation. The amplitude for the negative damped case will gradually grow unbounded for both the sequences making the network unstable. Thus, we need to set the right inductive bias in the model for it to be appropriately damped in order to foster long range information propagation capability.
</p>
<p>
  We enforce the condition above making use of Lemma 3.2 in Ref <d-cite bibtex-key="orvieto2023resurrecting"></d-cite> which is given as follows:
  Let $$u_1,u_2$$ be independent uniform random variables on the interval $$[0,1]$$. Let $$0\le r_{\min}\le r_{\max}\le1$$. Compute $$\nu = -\frac{1}{2}\log\left(u_1(r_{\max}^2-r_{\min}^2)+r_{\min}^2\right)$$ and $$\theta = 2\pi u_2 $$. Then $$\exp(-\nu+i\theta)$$ is uniformly distributed on the ring in complex plane $$\mathbb{C}$$ between circles of radii $$r_{\min}$$ and $$r_{\max}$$.<aside>The Glorot initialization naturally places the eigenvalues within the unit circle in the complex plane.</aside>
  </p>
  <d-figure>
    <img src="exponentialinitialization.png" style="height:200px;width:250px;display: block; margin-left: auto; margin-right: auto;margin-top:10px;"/>
    <figcaption style="text-align:center;">
      <a class="marker" href="#exponentiali">
        Figure 6: Exponential initialization of eigenvalues within a ring.
      </a>
    </figcaption>
  </d-figure>
  <p>
  This is shown in the above figure with $$r_{min} = 0.4$$ and $$r_{max} = 0.9$$.
  This suggests a natural parameterization for $$A$$ as $$\Lambda = \text{diag}(\exp(-\nu + i \theta))$$ with $$\nu$$ and $$\theta$$ as the learnable parameters.
</p>
<p>
  Exponential parameterization only solves half of the problem by keeping the eigenvalues confined within
  a ring inside the unit circle and therefore bounded. We also need to ensure the stability of the dynamics discussed in spring-mass-damper example by having a positive damping. We need to ensure that $$\nu > 0$$ (this ensures positive damping similar to $$c > 0$$ in the spring-mass-damper system).
  This is easy to do if we use another positive nonlinearity in the form of exponential: $$\lambda_j:=\exp(-\exp(\nu_j^{\log})+i\theta_j)$$, where $$ \nu^{\log}_j $$ is the parameter that is optimized. We can see from Equation <a href="#diagrecurrence">(5)</a> having the term $$\Lambda^j$$ that we need to initialize the eigenvalue closer to the unit disk by making $$r_{min}$$ closer to 1 ($$\lambda_j^k \approx 0$$ when $$\lambda_j \ll 1$$) for deep information propagation and model long range interactions. 
</p>
<a class="marker" href="#transientdynamics" id="transientdynamics"></a>
<h4>Designing the transient dynamics through small phase</h4>
<p>
  Additionally, in the studies in Ref <d-cite bibtex-key="orvieto2023resurrecting"></d-cite> it was also found that the LRU has to be initialized with a small phase $$\theta_j \approx [0,\pi/50]$$ to be able to do well on the most difficult task of Path-X.
<p>
<div style="display: flex; justify-content: space-around; align-items: center;">
  <d-figure>
      <img src="sample_128_positive.png" style="height:270px;width:270px;margin-top:10px;">
      <figcaption style="text-align:center; margin-top:10px;">
        Path-X positive sample.
      </figcaption>
  </d-figure>
  <d-figure>
      <img src="sample_128_negative.png" style="height:270px;width:270px;margin-top:10px;">
      <figcaption style="text-align:center; margin-top:10px;">
        Path-X negative sample.
      </figcaption>
  </d-figure>
</div>
<p>
The figure above shows the positive and negative samples for the Path-X task. The image consists of 16000 pixels which are fed sequentially to the model. The task is to ascertain if the two white dots are connected by a path. Unconstrained initialization between $$[0,2\pi]$$ can result in high frequency dynamics with large number of oscillations (see Figure 4 in Ref <d-cite bibtex-key="orvieto2023resurrecting"></d-cite>). <em>The presence of high frequency components makes the system quickly settle into its autonomous modes and the memory of the input is lost. We would like the dynamics to stay transient and driven by the inputs (see Ref <d-cite bibtex-key="PhysRevLett.108.244101"></d-cite>).</em> 
</p>

<h4>The full neural network architecture</h4>
<d-figure>
  <img src="liru1.jpg" style="height:400px;width:700px;display: block; margin-left: auto; margin-right: auto;margin-top:10px;"/>
  <figcaption style="text-align:center; margin-top:10px;">
    <a class="marker" href="#linearrecurrentunitnetwork">
      Figure 7: The complete architecture for LRU. This is Figure 1 from Ref <d-cite bibtex-key="orvieto2023resurrecting"></d-cite>.
    </a>
  </figcaption>
</d-figure>
<p>
  The complete architecture for the entire network is shown in Figure <a href="#linearrecurrentunitnetwork">(7)</a>.
  The $$M$$ dimensional input of length $$L$$ is projected into a dimension $$H$$ (see Section <a href="#koopmanoperator">Koopman operator</a>). The layer normalization or batch normalization is applied before passing it on to LRU. The LRU outputs are further processed using GLU and skip connection.
</p>
<h3>Why use linear recurrent block?</h3>
<p>
  Given all the discussions so far, let us summarize as to why we want to use a linear recurrent block.
<ul>
  <li>Transformers are inefficient with $$\mathcal{O}(L^2)$$ complexity during training and inference.</li>
  <li>RNNs have $$\mathcal{O}(L)$$ complexity during training and can do inference in constant time.</li>
  <li>A nonlinear RNN is difficult to train due to bifurcations encountered during training (see Section <a href="#difficultyrnn">The difficulty of training RNN</a>).</li>
  <li>The nonlinear RNN requires unrolling one time-step at a time and therefore cannot be parallelized.</li>
  <li>The nonlinearities added before the linear recurrent blocks are sufficient to ensure modeling fidelity and afford universal computation according to Koopman operator theory (see Section <a href="#koopmanoperator">Koopman Operator</a>).</li>
  <li>The linear recurrence can be computed in one single step by recursively expressing the state over the time range as a function of input. It is possible to relate the final state to the input sequence with a simple equation without the need to compute the intermediate state (see Equation <a href="#scanrecurrence">(4)</a> in Section <a href="#designingrnn">Designing Linear Recurrent Unit</a>).</li>
  <li>Every non-diagonal state space model can be diagonalized with a reparametrization with potentially complex entries. The diagonalized $$A$$ offers additional computational efficiency using parallel scans<d-cite bibtex-key="martin2017parallelizing"></d-cite> (see Section <a href="#diagonalizinga">Diagaonalizing $$A$$</a>).</li>
  <li>We can design the stability characteristics of the linear system to be appropriately stable and ensure stability during training giving us the ability to model long sequences with fast training and inference (see Section <a href="#stableexponentialparametrization">Stable exponential parametrization</a>).</li>
  <li>Keeping the imaginary part of the eigenvalue $$\theta$$ also known as phase is necessary to ensure that oscillations are driven by inputs and hence encode the information about the input (see Section <a href="#transientdynamics">Designing the transient dynamics</a>).</li>
</ul>
</p>

<h3>Further thoughts: Resurrecting nonlinear recurrent units on hardware</h3>
<p>
  We saw in the Section <a href="#transientdynamics">Designing the transient dynamics</a> that it was necessary to keep the phase small (imaginary part of the eigenvalue) to solve the most difficult task of Path-X in the long range arena benchmark.
  </p>
  <d-figure>
    <img src="photonicnonlineartransientcomputing.png" style="height:150px;width:550px;display: block; margin-left: auto; margin-right: auto;margin-top:10px;"/>
  </d-figure>
  <p>
  Thus, the information and memory about the sequence is stored in the transient dynamics
  of the linear dynamical system that we designed. We designed these transients to be stable by creating an inductive bias that places the real part of the eigenvalue in the left of the complex plane.
  The transients will blow up if the system is unstable (see Figure <a href="#springmassdynamics">(4)</a> and <a href="#springmassdynamics1">(5)</a>). The linear state space models also afforded fast computation through the use of Equation <a href="#diagrecurrence">(5)</a> and parallel scans. It is possible to reap all of the above benefits through the use of certain class of nonlinear dynamical systems that can also be implemented on hardwares  that are fast, energy efficient, and, have very high bandwidth<d-cite bibtex-key="brunner2013parallel"></d-cite>. These systems are both unstable and stable at the same time. They are composed of multiple modes many of which are unstable. However, their oscillations are still bounded due to the presence of nonlinearities. They have tremendous information processing capacity<d-cite bibtex-key="appeltant2011information"></d-cite> because they are repositories of rich dynamical patterns in the form of transients that can serve as an effective kernel.
  A photonics based hardware can process million words per second<d-cite bibtex-key="PhysRevX.7.011015"></d-cite>. There is also no need to train these nonlinear recurrent units on hardware. The only trainable components would be preprocessing and post processing components placed before and after these nonlinear recurrent unit blocks. 
</p>
</d-article>

<d-appendix>
  <style>
  
  d-appendix {
    contain: layout style;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-top: 60px;
    margin-bottom: 0;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    color: rgba(0,0,0,0.5);
    padding-top: 60px;
    padding-bottom: 48px;
  }
  
  d-appendix h3 {
    grid-column: page-start / text-start;
    font-size: 15px;
    font-weight: 500;
    margin-top: 1em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.65);
  }
  
  d-appendix h3 + * {
    margin-top: 1em;
  }
  
  d-appendix ol {
    padding: 0 0 0 15px;
  }
  
  @media (min-width: 768px) {
    d-appendix ol {
      padding: 0 0 0 30px;
      margin-left: -30px;
    }
  }
  
  d-appendix li {
    margin-bottom: 1em;
  }
  
  d-appendix a {
    color: rgba(0, 0, 0, 0.6);
  }
  
  d-appendix > * {
    grid-column: text;
  }
  
  d-appendix > d-footnote-list,
  d-appendix > d-citation-list,
  d-appendix > distill-appendix {
    grid-column: screen;
  }
  
  </style>
  
  
    <style>
    
    d-appendix {
      contain: layout style;
      font-size: 0.8em;
      line-height: 1.7em;
      margin-top: 60px;
      margin-bottom: 0;
      border-top: 1px solid rgba(0, 0, 0, 0.1);
      color: rgba(0,0,0,0.5);
      padding-top: 60px;
      padding-bottom: 48px;
    }
    
    d-appendix h3 {
      grid-column: page-start / text-start;
      font-size: 15px;
      font-weight: 500;
      margin-top: 1em;
      margin-bottom: 0;
      color: rgba(0,0,0,0.65);
    }
    
    d-appendix h3 + * {
      margin-top: 1em;
    }
    
    d-appendix ol {
      padding: 0 0 0 15px;
    }
    
    @media (min-width: 768px) {
      d-appendix ol {
        padding: 0 0 0 30px;
        margin-left: -30px;
      }
    }
    
    d-appendix li {
      margin-bottom: 1em;
    }
    
    d-appendix a {
      color: rgba(0, 0, 0, 0.6);
    }
    
    d-appendix > * {
      grid-column: text;
    }
    
    d-appendix > d-footnote-list,
    d-appendix > d-citation-list,
    d-appendix > distill-appendix {
      grid-column: screen;
    }
    
    </style>
    
    
      <d-bibliography src="bibliography.bib"></d-bibliography>
    <d-footnote-list style="display: none;">
  <style>
  
  d-footnote-list {
    contain: layout style;
  }
  
  d-footnote-list > * {
    grid-column: text;
  }
  
  d-footnote-list a.footnote-backlink {
    color: rgba(0,0,0,0.3);
    padding-left: 0.5em;
  }
  
  </style>
  
  <h3>Footnotes</h3>
  <ol></ol>
  
    <style>
    
    d-footnote-list {
      contain: layout style;
    }
    
    d-footnote-list > * {
      grid-column: text;
    }
    
    d-footnote-list a.footnote-backlink {
      color: rgba(0,0,0,0.3);
      padding-left: 0.5em;
    }
    
    </style>
    
    <h3>Footnotes</h3>
    <ol></ol>
    </d-footnote-list><d-citation-list style=""><style>
    d-citation-list {
      contain: style;
    }
    
    d-citation-list .references {
      grid-column: text;
    }
    
    d-citation-list .references .title {
      font-weight: 500;
    }
    </style><h3 id="references">References</h3><ol id="references-list" class="references"><li id="tay2020long"><span class="title">Long range arena: A benchmark for efficient transformers</span> <br>Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S. and Metzler, D., 2020. arXiv preprint arXiv:2011.04006. </li><li id="nichkawde2004stability"><span class="title">Stability analysis of a multibody system model for coupled slosh--vehicle dynamics</span> <br>Nichkawde, C., Harish, P. and Ananthkrishnan, N., 2004. Journal of Sound and Vibration, Vol 275(3-5), pp. 1069--1083. Elsevier.</li><li id="axler1997linear"><span class="title">Linear algebra done right</span> <br>Axler, S., 1997. Springer Science &amp; Business Media.</li><li id="pascanu2013difficulty"><span class="title">On the difficulty of training Recurrent Neural Networks</span> <br>Pascanu, R., Mikolov, T. and Bengio, Y., 2013. </li><li id="koopman1932dynamical"><span class="title">Dynamical systems of continuous spectra</span> <br>Koopman, B.O. and Neumann, J.v., 1932. Proceedings of the National Academy of Sciences, Vol 18(3), pp. 255--263. National Acad Sciences.</li><li id="lusch2018deep"><span class="title">Deep learning for universal linear embeddings of nonlinear dynamics</span> <br>Lusch, B., Kutz, J.N. and Brunton, S.L., 2018. Nature communications, Vol 9(1), pp. 4950. Nature Publishing Group UK London.</li><li id="orvieto2023resurrecting"><span class="title">Resurrecting recurrent neural networks for long sequences</span> <br>Orvieto, A., Smith, S.L., Gu, A., Fernando, A., Gulcehre, C., Pascanu, R. and De, S., 2023. arXiv preprint arXiv:2303.06349. </li><li id="gu2021efficiently"><span class="title">Efficiently modeling long sequences with structured state spaces</span> <br>Gu, A., Goel, K. and Re, C., 2021. arXiv preprint arXiv:2111.00396. </li><li id="smith2022simplified"><span class="title">Simplified state space layers for sequence modeling</span> <br>Smith, J.T., Warrington, A. and Linderman, S.W., 2022. arXiv preprint arXiv:2208.04933. </li><li id="gu2023mamba"><span class="title">Mamba: Linear-time sequence modeling with selective state spaces</span> <br>Gu, A. and Dao, T., 2023. arXiv preprint arXiv:2312.00752. </li><li id="martin2017parallelizing"><span class="title">Parallelizing linear recurrent neural nets over sequence length</span> <br>Martin, E. and Cundy, C., 2017. arXiv preprint arXiv:1709.04057. </li><li id="PhysRevLett.108.244101"><span class="title">Photonic Nonlinear Transient Computing with Multiple-Delay Wavelength Dynamics</span>   <a href="https://link.aps.org/doi/10.1103/PhysRevLett.108.244101">[link]</a><br>Martinenghi, R., Rybalko, S., Jacquot, M., Chembo, Y.K. and Larger, L., 2012. Phys. Rev. Lett., Vol 108(24), pp. 244101. American Physical Society. <a href="https://doi.org/10.1103/PhysRevLett.108.244101" style="text-decoration:inherit;">DOI: 10.1103/PhysRevLett.108.244101</a></li><li id="brunner2013parallel"><span class="title">Parallel photonic information processing at gigabyte per second data rates using transient states</span> <br>Brunner, D., Soriano, M.C., Mirasso, C.R. and Fischer, I., 2013. Nature communications, Vol 4(1), pp. 1364. Nature Publishing Group UK London.</li><li id="appeltant2011information"><span class="title">Information processing using a single dynamical node as complex system</span> <br>Appeltant, L., Soriano, M.C., Van der Sande, G., Danckaert, J., Massar, S., Dambre, J., Schrauwen, B., Mirasso, C.R. and Fischer, I., 2011. Nature communications, Vol 2(1), pp. 468. Nature Publishing Group UK London.</li><li id="PhysRevX.7.011015"><span class="title">High-Speed Photonic Reservoir Computing Using a Time-Delay-Based Architecture: Million Words per Second Classification</span>   <a href="https://link.aps.org/doi/10.1103/PhysRevX.7.011015">[link]</a><br>Larger, L., Baylon-Fuentes, A., Martinenghi, R., Udaltsov, V.S., Chembo, Y.K. and Jacquot, M., 2017. Phys. Rev. X, Vol 7(1), pp. 011015. American Physical Society. <a href="https://doi.org/10.1103/PhysRevX.7.011015" style="text-decoration:inherit;">DOI: 10.1103/PhysRevX.7.011015</a></li></ol></d-citation-list></d-appendix>

<!-- Default Statcounter code for State Space Sequence
Models
http://cnichkawde.github.io/statespacesequencemodels.html
-->
<script type="text/javascript">
  var sc_project=12959761; 
  var sc_invisible=1; 
  var sc_security="f194bfed"; 
  </script>
  <script type="text/javascript"
  src="https://www.statcounter.com/counter/counter.js"
  async></script>
  <noscript><div class="statcounter"><a title="website
  statistics" href="https://statcounter.com/"
  target="_blank"><img class="statcounter"
  src="https://c.statcounter.com/12959761/0/f194bfed/1/"
  alt="website statistics"
  referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
  <!-- End of Statcounter Code -->
    
</body>